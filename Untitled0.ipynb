{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOFB4J1+RaRo+xFrHIHVpsT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HEduvqjhk79m","executionInfo":{"status":"ok","timestamp":1713588569410,"user_tz":-330,"elapsed":78358,"user":{"displayName":"Shivansh Tripathi","userId":"14762717759276034221"}},"outputId":"306fa6fe-17cf-4cc9-dddb-c7ea797456e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.2-py3-none-any.whl (750 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.8/750.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.17.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Collecting thop>=0.1.1 (from ultralytics)\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 thop-0.1.1.post2209072238 ultralytics-8.2.2\n"]}],"source":["# Installing  the Ultralytics package\n","# This package provides pre-trained models and utilities for computer vision tasks\n","!pip install ultralytics"]},{"cell_type":"code","source":["# Import the YOLO model from the Ultralytics package\n","from ultralytics import YOLO\n","\n","# Import the os module for interacting with the operating system\n","import os\n","\n","# Import the display and Image modules from IPython for displaying images\n","from IPython.display import display, Image\n","\n","# Import the display module from IPython\n","from IPython import display\n","\n","# Clear any previous output from the notebook\n","display.clear_output()\n","\n","# Run the YOLO model in \"checks\" mode\n","# This mode performs various checks and validations on the model and data\n","!yolo mode=checks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TAlFSfQNlE0g","executionInfo":{"status":"ok","timestamp":1713588585407,"user_tz":-330,"elapsed":10679,"user":{"displayName":"Shivansh Tripathi","userId":"14762717759276034221"}},"outputId":"6dab1902-7b7d-40a8-b008-39e9721e0522"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/bin/yolo\", line 8, in <module>\n","    sys.exit(entrypoint())\n","  File \"/usr/local/lib/python3.10/dist-packages/ultralytics/cfg/__init__.py\", line 523, in entrypoint\n","    raise ValueError(f\"Invalid 'mode={mode}'. Valid modes are {MODES}.\\n{CLI_HELP_MSG}\")\n","ValueError: Invalid 'mode=<module 'ultralytics.utils.checks' from '/usr/local/lib/python3.10/dist-packages/ultralytics/utils/checks.py'>'. Valid modes are {'predict', 'val', 'benchmark', 'train', 'export', 'track'}.\n","\n","    Arguments received: ['yolo', 'mode=checks']. Ultralytics 'yolo' commands use the following syntax:\n","\n","        yolo TASK MODE ARGS\n","\n","        Where   TASK (optional) is one of {'classify', 'pose', 'segment', 'detect', 'obb'}\n","                MODE (required) is one of {'predict', 'val', 'benchmark', 'train', 'export', 'track'}\n","                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n","                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n","\n","    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n","        yolo train data=coco8.yaml model=yolov8n.pt epochs=10 lr0=0.01\n","\n","    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n","        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n","\n","    3. Val a pretrained detection model at batch-size 1 and image size 640:\n","        yolo val model=yolov8n.pt data=coco8.yaml batch=1 imgsz=640\n","\n","    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n","        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n","\n","    6. Explore your datasets using semantic search and SQL with a simple GUI powered by Ultralytics Explorer API\n","        yolo explorer\n","\n","    5. Run special commands:\n","        yolo help\n","        yolo checks\n","        yolo version\n","        yolo settings\n","        yolo copy-cfg\n","        yolo cfg\n","\n","    Docs: https://docs.ultralytics.com\n","    Community: https://community.ultralytics.com\n","    GitHub: https://github.com/ultralytics/ultralytics\n","    \n"]}]},{"cell_type":"code","source":["# Install the Roboflow package\n","!pip install roboflow\n","\n","# Import the Roboflow class from the roboflow module\n","from roboflow import Roboflow\n","\n","# Create an instance of the Roboflow class with your API key\n","rf = Roboflow(api_key=\"1lix18hpqDEsICKrb9jM\")\n","\n","# Access the project \"ui-screenshots\" in the workspace \"webuiproject\"\n","project = rf.workspace(\"webuiproject\").project(\"ui-screenshots\")\n","\n","# Get the first version of the project\n","version = project.version(1)\n","\n","# Download the dataset for the YOLOv8-OBB format\n","dataset = version.download(\"yolov8-obb\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7jeKzF3jlOfj","executionInfo":{"status":"ok","timestamp":1713588613787,"user_tz":-330,"elapsed":22120,"user":{"displayName":"Shivansh Tripathi","userId":"14762717759276034221"}},"outputId":"6a161bd9-e8cd-4e67-ee88-ea8c429bcff0"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting roboflow\n","  Downloading roboflow-1.1.27-py3-none-any.whl (74 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.1/74.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting certifi==2023.7.22 (from roboflow)\n","  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/158.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting chardet==4.0.0 (from roboflow)\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cycler==0.10.0 (from roboflow)\n","  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n","Collecting idna==2.10 (from roboflow)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n","Collecting opencv-python-headless==4.8.0.74 (from roboflow)\n","  Downloading opencv_python_headless-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Collecting python-dotenv (from roboflow)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.2)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n","Collecting requests-toolbelt (from roboflow)\n","  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-magic (from roboflow)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.51.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n","Installing collected packages: python-magic, python-dotenv, opencv-python-headless, idna, cycler, chardet, certifi, requests-toolbelt, roboflow\n","  Attempting uninstall: opencv-python-headless\n","    Found existing installation: opencv-python-headless 4.9.0.80\n","    Uninstalling opencv-python-headless-4.9.0.80:\n","      Successfully uninstalled opencv-python-headless-4.9.0.80\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.7\n","    Uninstalling idna-3.7:\n","      Successfully uninstalled idna-3.7\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.12.1\n","    Uninstalling cycler-0.12.1:\n","      Successfully uninstalled cycler-0.12.1\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 5.2.0\n","    Uninstalling chardet-5.2.0:\n","      Successfully uninstalled chardet-5.2.0\n","  Attempting uninstall: certifi\n","    Found existing installation: certifi 2024.2.2\n","    Uninstalling certifi-2024.2.2:\n","      Successfully uninstalled certifi-2024.2.2\n","Successfully installed certifi-2023.7.22 chardet-4.0.0 cycler-0.10.0 idna-2.10 opencv-python-headless-4.8.0.74 python-dotenv-1.0.1 python-magic-0.4.27 requests-toolbelt-1.0.0 roboflow-1.1.27\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["certifi","chardet","cv2","cycler","idna"]},"id":"be53a81b5690467da940baad87c8a52a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["loading Roboflow workspace...\n","loading Roboflow project...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading Dataset Version Zip in UI-screenshots-1 to yolov8-obb:: 100%|██████████| 91055/91055 [00:03<00:00, 26082.88it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["\n","Extracting Dataset Version Zip to UI-screenshots-1 in yolov8-obb:: 100%|██████████| 3612/3612 [00:00<00:00, 4870.39it/s]\n"]}]},{"cell_type":"code","source":["!yolo task=detect mode=train model=yolov8m.pt data=\"/content/UI-screenshots-1/data.yaml\" epochs=100 imgsz=640\n","\n","\"\"\"\n","Run YOLOv8 object detection model training\n","!yolo task=detect mode=train: This command runs the YOLOv8 object detection model in training mode\n","model=yolov8m.pt: This parameter specifies the path to the YOLOv8 model file, in this case, 'yolov8m.pt', which is the pre-trained YOLOv8 model\n","data=\"/content/UI-screenshots-1/data.yaml\": This parameter points to the data configuration file ('data.yaml') located in the '/content/UI-screenshots-1/' directory. This file contains information about the dataset, such as paths to the training and validation data, class labels, and other dataset-specific settings\n","epochs=100: This parameter sets the number of training epochs to 100. An epoch is one complete pass through the entire training dataset\n","imgsz=640: This parameter specifies the input image size for the model during training. In this case, the images will be resized to 640x640 pixels before being fed into the model\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EN-BAAxLlb1A","executionInfo":{"status":"ok","timestamp":1713589711299,"user_tz":-330,"elapsed":94427,"user":{"displayName":"Shivansh Tripathi","userId":"14762717759276034221"}},"outputId":"b856b9dc-1ab1-43cf-d99e-341baecf408f"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics YOLOv8.2.2 🚀 Python-3.10.12 torch-2.2.1+cu121 CPU (Intel Xeon 2.20GHz)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/content/UI-screenshots-1/data.yaml, epochs=100, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n","2024-04-20 05:07:09.503117: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-20 05:07:09.503191: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-20 05:07:09.506231: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","Overriding model.yaml nc=80 with nc=8\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n","  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n","  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n","  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n","  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n","  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n","  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n","  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n","  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n","  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n"," 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n"," 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n"," 22        [15, 18, 21]  1   3780328  ultralytics.nn.modules.head.Detect           [8, [192, 384, 576]]          \n","Model summary: 295 layers, 25860952 parameters, 25860936 gradients, 79.1 GFLOPs\n","\n","Transferred 469/475 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/UI-screenshots-1/train/labels.cache... 1282 images, 3 backgrounds, 0 corrupt: 100% 1282/1282 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/9to5mac_com_png_jpg.rf.666e4d8e3fb2bf372bb56211dd63c8bf.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/artstation_com_png_jpg.rf.e6788fe2e6baa55609e46d3170e33e41.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/bandsintown_com_png_jpg.rf.cb9ce770b3773bf5dfcccd703703d23c.jpg: 4 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/bangordailynews_com_png_jpg.rf.ffd5249679d9fe0a5116e40b8d3ecf81.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/behance_net_png_jpg.rf.a1d66b87f5a43f1f7a15b8d544077d9b.jpg: 14 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/bing_com_png_jpg.rf.7944c47ef9b8fac349a748af1b74a3ae.jpg: 24 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/bitmex_com_png_jpg.rf.fe032235b394d227d97d49e431329ca0.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/business_com_png_jpg.rf.aae106795c0ba889ab2cffb9e4f84715.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/calameo_com_png_jpg.rf.b15d4c0f56f6eecd1105ce40024a7bac.jpg: 10 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/class_ruten_com_tw_png_jpg.rf.ac48f539f47009d152ce2cd873e0a09d.jpg: 8 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/clickondetroit_com_png_jpg.rf.ba2472f6d424f81088078df4283787be.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/cnn_com_png_jpg.rf.0538103be3ca6fc00dd9609e49e245fb.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/collegehumor_com_png_jpg.rf.b126927a742471ba9ed529701ab2e30b.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/commons_wikimedia_org_png_jpg.rf.7ce5beb8b2b6af7aa13257adbf74d457.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/coursera_org_png_jpg.rf.6fb7c7313294fe0558d198b512e1a9e7.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/deondernemer_nl_png_jpg.rf.43df0fb6a5531a6692ae1c3bd7315d2f.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/edition_cnn_com_png_jpg.rf.54ca00ba56b72dd870160c1ee125f419.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/elle_com_png_jpg.rf.d3fe9622b88cd571dfcc0e08d7ce3456.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/eplus_jp_png_jpg.rf.5b92459cd96debf9165d33cbbbc91229.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/es_wikipedia_org_png_jpg.rf.7e5e945684c20c4ddf1b5ad30a1ef832.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/esa_int_png_jpg.rf.35a77e90808cd05adb6507369ffb120b.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/explorestlouis_com_png_jpg.rf.cac02188329218e6250361f66dda521c.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/fastcompany_com_png_jpg.rf.fae21ef28ec1daa7bf0b5ee5d8917871.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/g1_globo_com_png_jpg.rf.3e4b72f0403c66ba02a4c924c50c7a25.jpg: 1645 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/gofundme_com_png_jpg.rf.877ca53025b1d0c39c809c22b069b780.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/gq_com_png_jpg.rf.98c739f8f7f41d49574e77ed2ab8350d.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/greenpeace_org_png_jpg.rf.d0bbd0b3aee54bf6bc5ce90a943a20d5.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/guardian_co_uk_png_jpg.rf.fef60a14b1542d09a2d091e72e787780.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/hangouts_google_com_png_jpg.rf.2b0bcbb11067e6c57c690620d184ac6b.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/homedepot_com_png_jpg.rf.8be4c0fd1dbadbe077a4237b91353bd1.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/houzz_com_png_jpg.rf.40305a5286a1e55677790f13732a9eb7.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/icloud_com_png_jpg.rf.75ac1d61186b41b387d6f7ec7828f377.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/imdb_com_png_jpg.rf.5fc829cc1ca3ee0261f9d57294bef106.jpg: 4 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/issuu_com_png_jpg.rf.190a43fc0ee3d905e0ae07c5a9975aa1.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/it_wikipedia_org_png_jpg.rf.edd16288dd8e9a00e627188d83562797.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/ja_wikipedia_org_png_jpg.rf.7504d99adaa5a99d9a99363cbbf52164.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/jotform_com_png_jpg.rf.fab0c44e815a01e52697991edb73d760.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/jsfiddle_net_png_jpg.rf.0009cb9b1b5e53436dd2be032f775b5a.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/kritzelpixel_de_png_jpg.rf.c9e5b9ca5ce0cfd8bd799c56d7183301.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/madmimi_com_png_jpg.rf.2f5b74af97f8d684d637dec38db910f8.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/mdpi_com_png_jpg.rf.6e88a564b33d46a02fbd7c418c1ae086.jpg: 7 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/mercadopago_com_png_jpg.rf.47ae2fb377b819b4c139c0844f65fa04.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/meta_wikimedia_org_png_jpg.rf.fde809f0fa669a989413a35d1a020c23.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/mix_com_png_jpg.rf.bde941c35c9ae8611c8a9795fc5a40af.jpg: 4 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/nbc_com_png_jpg.rf.6955f51a5ccd8dec78c4a9168acabb71.jpg: 5 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/nbcnews_com_png_jpg.rf.cfd5b53d0cd0177b86d15a63140a78ac.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/ncbi_nlm_nih_gov_png_jpg.rf.3329727f5c7a47408b640d50a68ff854.jpg: 5 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/netflix_com_png_jpg.rf.a8ae992b258a1fa82efe82a713e8cbef.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/news_bbc_co_uk_png_jpg.rf.4e5166700b40bccc3d078f358800d1a0.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/news_nationalgeographic_com_png_jpg.rf.8200b4367bed250c0f050860f1e15391.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/news_yahoo_com_png_jpg.rf.018b76f2bcc5eb4ab44f932f04b2ffd1.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/nicovideo_jp_png_jpg.rf.3819b25ca940575bc09bfc4e6a6304fb.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/nofilmschool_com_png_jpg.rf.c798d03e1cf493e5a631f2a7e557423a.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/nytimes_com_png_jpg.rf.64af1b863d7234e1cac3cb145e8baa1b.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/openoffice_org_png_jpg.rf.7ea7e34d4445be36588d984da11fb917.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/pitchfork_com_png_jpg.rf.0bfa32d05dff40d8a581d41a598166a9.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/pt_wikipedia_org_png_jpg.rf.f0ae39f4e24c94884497f843df8eae62.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/rp-online_de_png_jpg.rf.17aebffb34ad45f75d7da1752bdcbf50.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/s0_wp_com_png_jpg.rf.58f2b2a540368459c24b2475e537abd7.jpg: 20 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/scmp_com_png_jpg.rf.c27517ede50d8534eab08677116c7fca.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/secure_php_net_png_jpg.rf.ab5392b34d7e240a2b8d2799fbb23e53.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/shop_trezor_io_png_jpg.rf.d4674375d3da0f42cae423cabd42768d.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/speakerdeck_com_png_jpg.rf.2f41ff73898ab554eb357d10d19e80f7.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/stumbleupon_com_png_jpg.rf.9707622e15cf58b159fcd5d0d14347b5.jpg: 4 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/sublimetext_com_png_jpg.rf.4a9edd9aafa86f6559d4f6c16aba8739.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/sv_wikipedia_org_png_jpg.rf.abfa4d46dd7ef849661349c82ffad19f.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/ted_com_png_jpg.rf.b5d2292b6aee0c4752d36496d7f07f71.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/theculturetrip_com_png_jpg.rf.0df9c6b87e2a00a7faeab47f15ecd36d.jpg: 5 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/uk_wikipedia_org_png_jpg.rf.ce1a998626581be8c61e9566285b830b.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/unesco_org_png_jpg.rf.62b011e7016a0f15d063c650829363f5.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/upload_wikimedia_org_png_jpg.rf.91436744565b113f011897743e37bda2.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/us_battle_net_png_jpg.rf.f26fb3e43b8be30c5e106ac050992937.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/vedomosti_ru_png_jpg.rf.47d4dd683ed83dd35cf53a15654dffb7.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/vice_com_png_jpg.rf.502db047048f3dcfc7b831b61a014f72.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/whitehouse_gov_png_jpg.rf.54b93369b219819e1ef83e01a923afe2.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/wp_me_png_jpg.rf.95adb91c9b7669b7fa86e0b1f793d68f.jpg: 20 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/www1_folha_uol_com_br_png_jpg.rf.6aa871411dc294a681a5b6335224bf97.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/www3_nhk_or_jp_png_jpg.rf.ce48974ee1c56af64919801260f1199d.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/yr_no_png_jpg.rf.4f5d0e2597e27f0d88be5b8bb066acac.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/train/images/zen_yandex_ru_png_jpg.rf.858dbf15372bc56c5e7ee38636786da7.jpg: 6 duplicate labels removed\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/UI-screenshots-1/valid/labels.cache... 339 images, 0 backgrounds, 0 corrupt: 100% 339/339 [00:00<?, ?it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/aka_ms_png_jpg.rf.f3793f3603778301e9f083f934bf983c.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/bbc_com_png_jpg.rf.45ec1d027dd60934a50d5aae6209155f.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/blockchain_info_png_jpg.rf.19f7a1b1e46142c462fa7acec46d5419.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/coindesk_com_png_jpg.rf.e1f2938d275ea8e93844a96d1a479815.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/de_wikipedia_org_png_jpg.rf.8ca73ad4f907c715c2fce1d9d5ddc2e3.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/en_wikipedia_org_png_jpg.rf.ffaa58e3a8ee1a2a72c3cf2f063d9099.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/ey_com_png_jpg.rf.11b5a211ff429a14bcc8806b397fbe7c.jpg: 5 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/finance_yahoo_com_png_jpg.rf.7397bf4134cba91998e7783977a437bd.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/flickr_com_png_jpg.rf.6d42724eb83167d8c98f9d72fd6e556a.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/foxnews_com_png_jpg.rf.881c5ef7698a16d1211b21d2c7829f01.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/g2a_com_png_jpg.rf.1971c4f6448fbcb4fa4a12245dfa4f92.jpg: 2 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/justgiving_com_png_jpg.rf.ea6057af3daf63f3704c7b584603c8e8.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/paypal_com_png_jpg.rf.3b9569c092b03b5598cadf7e494eb80e.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/promods_net_png_jpg.rf.40d6bc4c71af86051dec661ca9ca3212.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/rt_com_png_jpg.rf.1b4363060b854c4661768dc9e3566d3b.jpg: 4 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/ru_wikipedia_org_png_jpg.rf.8eaa30f2384cf5e63056878752a99ca4.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/simple_wikipedia_org_png_jpg.rf.295cafeb93fd4762acb042c945b1974e.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/surveymonkey_com_png_jpg.rf.eb856a7dfddb6d5f76778d241b20c7dc.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/theguardian_com_png_jpg.rf.c2601a06b00f4c39d97ba8442fd87cc8.jpg: 3 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/theknot_com_png_jpg.rf.04d36dae6acf28c7bb4f3f1de1c8c861.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/today_com_png_jpg.rf.2f5dddf7dea9963a6b4f5dc4dd336a0a.jpg: 4 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/waze_com_png_jpg.rf.7db23321ccdd740e07a75a87da749af2.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/wordpress_com_png_jpg.rf.8b39d5def6529625416dd19c8b4c27cc.jpg: 20 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/yelp_com_png_jpg.rf.be94dee8bdb8271fbd56bea31d644ed9.jpg: 1 duplicate labels removed\n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/UI-screenshots-1/valid/images/yummly_com_png_jpg.rf.dd0e5bd8f60bd5d7f2026fc2d703585a.jpg: 1 duplicate labels removed\n","Plotting labels to runs/detect/train3/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000833, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n","Image sizes 640 train, 640 val\n","Using 0 dataloader workers\n","Logging results to \u001b[1mruns/detect/train3\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","  0% 0/81 [00:00<?, ?it/s]^C\n"]},{"output_type":"execute_result","data":{"text/plain":["'\\nRun YOLOv8 object detection model training\\n!yolo task=detect mode=train: This command runs the YOLOv8 object detection model in training mode\\nmodel=yolov8m.pt: This parameter specifies the path to the YOLOv8 model file, in this case, \\'yolov8m.pt\\', which is the pre-trained YOLOv8 model\\ndata=\"/content/UI-screenshots-1/data.yaml\": This parameter points to the data configuration file (\\'data.yaml\\') located in the \\'/content/UI-screenshots-1/\\' directory. This file contains information about the dataset, such as paths to the training and validation data, class labels, and other dataset-specific settings\\nepochs=100: This parameter sets the number of training epochs to 100. An epoch is one complete pass through the entire training dataset\\nimgsz=640: This parameter specifies the input image size for the model during training. In this case, the images will be resized to 640x640 pixels before being fed into the model\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["\"\"\"\n","Test the trained YOLOv8 object detection model\n","\"\"\"\n","\n","# Import necessary libraries\n","from ultralytics import YOLO\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Load the trained model weights\n","model = YOLO(\"path/to/best.pt\")  # Replace with the path to your trained model weights\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Define a function to test the model on an image\n","def test_model(image_path):\n","    \"\"\"\n","    Test the trained YOLOv8 model on a given image.\n","\n","    Args:\n","        image_path (str): Path to the image file.\n","\n","    Returns:\n","        numpy.ndarray: Annotated image with bounding boxes and labels.\n","    \"\"\"\n","    # Load the image\n","    img = cv2.imread(image_path)\n","\n","    # Make a prediction\n","    results = model.predict(source=img, save=True)  # Optionally, save the annotated image\n","\n","    # Get the annotated image\n","    annotated_img = results[0].plot()\n","\n","    # Convert the annotated image to numpy array\n","    annotated_img = cv2.cvtColor(np.asarray(annotated_img), cv2.COLOR_RGB2BGR)\n","\n","    return annotated_img\n","\n","# Test the model on a sample image\n","test_image_path = \"path/to/test/image.jpg\"  # Replace with the path to your test image\n","annotated_image = test_model(test_image_path)\n","\n","# Display the annotated image\n","plt.figure(figsize=(10, 10))\n","plt.imshow(annotated_image)\n","plt.axis('off')\n","plt.show()\n","\n","# Calculate evaluation metrics\n","metrics = model.val()  # Assuming you have a separate validation dataset\n","\n","# Print the evaluation metrics\n","print(metrics)\n","\n","# Save the evaluation metrics\n","metrics.save(save_dir=\"path/to/metrics/directory\")  # Replace with the path to save metrics"],"metadata":{"id":"SuHd-taOpbOx"},"execution_count":null,"outputs":[]}]}